{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82ae4c8",
   "metadata": {},
   "source": [
    "\n",
    "## Whisper 모델 기반 금융 도메인 음성인식\n",
    "1. 목표\n",
    "\n",
    "  Whisper 모델을 파인튜닝하여 금융 도메인에 특화된 음성인식 모델을 생성\n",
    "\n",
    "2.  Dataset 개요\n",
    "- URL : https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=71557\n",
    "- 이름 : 뉴스 대본 및 앵커 음성 데이터\n",
    "- 언론에 보도된 뉴스기사, 각 분야(정치, 경제, 사회, 문화, 국제, 지역, 스포츠, IT과학)별 전직,현직 아나운서, 아나운서 교육생들이 뉴스를 보도하는 음성 데이터 1,132시간"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386e695",
   "metadata": {},
   "source": [
    "### [라이브러리 세부 설명]\n",
    "\n",
    "- `from transformers import WhisperProcessor, WhisperForConditionalGeneration`\n",
    ": Hugging Face에서 제공하는 라이브러리. Whisper와 같은 사전 학습된 모델을 로드하고 사용하기 위해 필요. WhisperProcessor는 오디오와 텍스트 데이터를 모델이 이해할 수 있는 형태로 변환하고, WhisperForConditionalGeneration은 실제 음성 인식 모델.\n",
    "\n",
    "- `from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer`\n",
    ": Hugging Face Transformers 라이브러리에서 모델을 학습시키는 데 필요한 도구. Seq2SeqTrainingArguments는 학습 설정을 정의하고, Seq2SeqTrainer는 실제 학습 과정을 관리.\n",
    "\n",
    "- `import evaluate`\n",
    ": Hugging Face에서 제공하는 라이브러리. 모델의 성능을 평가하는 다양한 지표(Metrics)를 제공. 여기서는 음성 인식의 성능을 측정하는 WER(단어 오류율)을 계산하기 위해 사용.\n",
    "\n",
    "- `import json`\n",
    ": JSON 형식의 데이터를 파싱하고 다룰 때 사용되는 파이썬 표준 라이브러리. 데이터셋의 메타데이터가 JSON 파일에 저장되어 있기 때문에 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbe69a",
   "metadata": {},
   "source": [
    "### [데이터셋 구조 파악]\n",
    "#### [데이터셋의 전체 디렉토리 구조]\n",
    "```\n",
    "data_root_path/\n",
    "├── Training/\n",
    "│   ├── 01_SourceData/TS/\n",
    "│   │   ├── file_a.wav\n",
    "│   │   ├── ...\n",
    "│   └── 02_LabeledData/TL/\n",
    "│       ├── file_b.json\n",
    "│       ├── ...\n",
    "│\n",
    "└── Validation/\n",
    "    ├── 01_SourceData/VS/\n",
    "    │   ├── file_c.wav\n",
    "    │   ├── ...\n",
    "    └── 02_LabeledData/VL/\n",
    "        ├── file_d.json\n",
    "        ├── ...\n",
    "```\n",
    "\n",
    "#### [레이블 구조 (JSON)]\n",
    "- 레이블 데이터는 JSON구조로 되어있다. \n",
    "아래처럼 스크립트 정보와 스피커정보, 그리고 오디오파일의 정보를 포함하고 있다.\n",
    "\n",
    "```\n",
    "{\n",
    "\t\"script\": {\n",
    "\t\t\"id\": \"YTNEC057\",\n",
    "\t\t\"url\": \"http://www.ytn.co.kr/_ln/0102_201801091444153396\",\n",
    "\t\t\"title\": \"최종구 `코스닥 활성화 위해 상장요건 완화·펀드 조성`\",\n",
    "\t\t\"press\": \"YTN\",\n",
    "\t\t\"press_field\": \"경제\",\n",
    "\t\t\"press_date\": \"20180109\",\n",
    "\t\t\"index\": 2,\n",
    "\t\t\"text\": \"최 위원장은 오늘 코스닥 시장 활성화 현장간담회에서 이 같은 내용을 담은 코스닥 활성화 방안을 공개했습니다.\",\n",
    "\t\t\"sentence_type\": \"작문형\",\n",
    "\t\t\"keyword\": \"코스닥,상장 요건,코스닥 상장,코스닥 활성화,위원장,코스닥 기업,최종구 코스닥,최종구 코스닥 활성화,활성화,코스닥 시장 활성화\"\n",
    "\t},\n",
    "\t\"speaker\": {\n",
    "\t\t\"id\": \"SPK054\",\n",
    "\t\t\"age\": \"20대\",\n",
    "\t\t\"sex\": \"남성\",\n",
    "\t\t\"job\": \"아나운서준비생\"\n",
    "\t},\n",
    "\t\"file_information\": {\n",
    "\t\t\"audio_format\": \"44100 Hz 16bit PCM\",\n",
    "\t\t\"utterance_start\": \"0.445\",\n",
    "\t\t\"utterance_end\": \"7.467\",\n",
    "\t\t\"audio_duration\": \"7.929\"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "#### [오디오 데이터 (WAV)]\n",
    "오디오 데이터는 JSON의 speaker필드 내에 있는 id값과 script필드 내의 id값의 조합 이름의 디렉토리에 .wav파일로 저장되어있다.\n",
    "\n",
    "예를 들어, 위 JSON 기준으로 `data['speaker']['id']`는 `SPK054`이고, `data['script']['id']`는 `YTNEC057`이다. \n",
    "\n",
    "그리고, `speaker`의 `sex`에 따라 데이터파일명의 `F` 또는 `M`이 결정되고, `script`의 `index`에 따라 성별 뒤 나오는 세자리수가 결정된다. \n",
    "\n",
    "최종적으로 위 JSON의 파일명은 `SPK054YTNEC057M002.json`이 되고, 이에 매칭되는 WAV파일의 이름은 `SPK054YTNEC057M002.wav`이 된다.\n",
    "\n",
    "이 규칙에 따라 JSON파일에서 필요한 정보를 파싱하고 해당 정보를 통해 매칭되는 오디오파일을 찾을 수 있다.\n",
    "\n",
    "\n",
    "예시) \n",
    "\n",
    "`...\\data\\open_data\\Validation\\02_LabeledData\\VL\\SPK054\\SPK054YTNEC057\\SPK054YTNEC057M001.json`\n",
    "`...\\data\\open_data\\Validation\\02_LabeledData\\VL\\SPK054\\SPK054YTNEC057\\SPK054YTNEC057M002.json`\n",
    "`...\\data\\open_data\\Validation\\02_LabeledData\\VL\\SPK054\\SPK054YTNEC057\\SPK054YTNEC057M003.json`\n",
    "`...\\data\\open_data\\Validation\\02_LabeledData\\VL\\SPK054\\SPK054YTNEC057\\SPK054YTNEC057M004.json`\n",
    "`...\\data\\open_data\\Validation\\02_LabeledData\\VL\\SPK054\\SPK054YTNEC057\\SPK054YTNEC057M005.json`\n",
    "`...\\data\\open_data\\Validation\\02_LabeledData\\VL\\SPK054\\SPK054YTNEC057\\SPK054YTNEC057M006.json`\n",
    "`...\\data\\open_data\\Validation\\02_LabeledData\\VL\\SPK054\\SPK054YTNEC057\\SPK054YTNEC057M007.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42717a",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "- 프로젝트의 목표는 위스퍼 모델을 경제/금융 도메인에 특화되도록 파인튜닝하여 성능을 높이는 것이므로, 데이터셋에서 경제 도메인만 필터링하는 과정이 필요하다. \n",
    "\n",
    "- 레이블링 데이터인 JSON파일에 도메인 정보를 포함하고 있어, 해당 부분을 파싱하여 경제 도메인이 내용인 경우에만 해당 데이터를 사용하도록 전처리를 해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b8a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cpu\n",
      "\n",
      "--- 검증 데이터 로드 시작 ---\n",
      "검증 데이터 오디오 경로: C:/Users/Admin/Desktop/github/voice-recognition-using-whisper/data/news_scripts_and_speech_data/open_data\\Validation\\01_SourceData\\VS\n",
      "검증 데이터 JSON 경로: C:/Users/Admin/Desktop/github/voice-recognition-using-whisper/data/news_scripts_and_speech_data/open_data\\Validation\\02_LabeledData\\VL\n",
      "총 1606개의 Validation용 경제 도메인 데이터를 찾았습니다.\n",
      "                                                path  \\\n",
      "0  C:/Users/Admin/Desktop/github/voice-recognitio...   \n",
      "1  C:/Users/Admin/Desktop/github/voice-recognitio...   \n",
      "2  C:/Users/Admin/Desktop/github/voice-recognitio...   \n",
      "3  C:/Users/Admin/Desktop/github/voice-recognitio...   \n",
      "4  C:/Users/Admin/Desktop/github/voice-recognitio...   \n",
      "\n",
      "                                       transcription  \n",
      "0  대부분 온라인 쇼핑몰에서 가공, 신선식품이나 일용 잡화를 판매할 때 단위 가격을 표...  \n",
      "1  이에 따라 소비자의 합리적인 선택을 위해 온라인 쇼핑몰에서도 오프라인 매장처럼 단위...  \n",
      "2  한국소비자원이 대형 마트 쇼핑몰 (3)/(세) 곳과 오픈 마켓 (8)/(여덟) 곳 ...  \n",
      "3  대형 마트 등 오프라인 매장은 판매 가격만으로는 가격 비교가 어려운 (84)/(여든...  \n",
      "4  소비자원이 쇼핑몰별로 (79~82)/(칠십 구 에서 팔십 이) 개 품목 각 (20)...  \n",
      "\n",
      "검증 데이터 전처리 중입니다...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8f7e47a7f741b0a037d89b6aba87ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e7f523927e40819d28625fbce52a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 후 검증 데이터셋 크기: 1606\n",
      "검증 데이터 전처리 완료.\n",
      "\n",
      "최종 모델을 평가합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [201/201 5:08:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 WER: 42.24%\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 장치: {device}\")\n",
    "\n",
    "def process_financial_news_data(base_data_path: str, data_type: str):\n",
    "    \"\"\"\n",
    "    레이블링된 뉴스 오디오 데이터를 로드하고, 도메인이 경제인 데이터만 필터링하여 데이터셋으로 반환해주는 함수 \n",
    "\n",
    "    Args:\n",
    "        base_data_path (str): 데이터셋의 베이스 경로\n",
    "        data_type (str): 데이터 유형 ('Training', 'Validation')\n",
    "    \"\"\"\n",
    "    audio_paths = []\n",
    "    transcriptions = []\n",
    "\n",
    "    # TS, TL, VS, VL 디렉토리 경로를 설정하기\n",
    "    if data_type == \"Training\":\n",
    "        audio_data_base_path = os.path.join(base_data_path, data_type, \"01_SourceData\", \"TS\")\n",
    "        json_data_path = os.path.join(base_data_path, data_type, \"02_LabeledData\", \"TL\")\n",
    "        print(f\"훈련 데이터 오디오 경로: {audio_data_base_path}\")\n",
    "        print(f\"훈련 데이터 JSON 경로: {json_data_path}\")\n",
    "    elif data_type == \"Validation\":\n",
    "        audio_data_base_path = os.path.join(base_data_path, data_type, \"01_SourceData\", \"VS\")\n",
    "        json_data_path = os.path.join(base_data_path, data_type, \"02_LabeledData\", \"VL\")\n",
    "        print(f\"검증 데이터 오디오 경로: {audio_data_base_path}\")\n",
    "        print(f\"검증 데이터 JSON 경로: {json_data_path}\")\n",
    "    \n",
    "    if not os.path.exists(json_data_path):\n",
    "        print(f\"경로를 찾을 수 없습니다: {json_data_path}\")\n",
    "        return None\n",
    "\n",
    "    # JSON 파일을 순회\n",
    "    for root, _, files in os.walk(json_data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                json_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # 경제 도메인만 필터링\n",
    "                    if data['script']['press_field'] == '경제':\n",
    "                        # JSON 파일에서 필요한 데이터만 파싱\n",
    "                        speaker_id = data['speaker']['id']\n",
    "                        script_id = data['script']['id']\n",
    "                        transcription = data['script']['text']\n",
    "                        sex = data['speaker']['sex']\n",
    "                        index = data['script']['index']\n",
    "\n",
    "                        sex_char = 'M' if sex == '남성' else 'F'\n",
    "                        index_padded = f\"{index:03d}\"\n",
    "\n",
    "                        audio_file_name = f\"{speaker_id}{script_id}{sex_char}{index_padded}.wav\"\n",
    "                        \n",
    "                        audio_path = os.path.join(audio_data_base_path, speaker_id, f\"{speaker_id}{script_id}\", audio_file_name)\n",
    "\n",
    "                        # 오디오 파일이 실제로 존재하는지 확인. 존재하면 해당 경로와 스크립트를 리스트에 append\n",
    "                        if os.path.exists(audio_path):\n",
    "                            audio_paths.append(audio_path)\n",
    "                            transcriptions.append(transcription)\n",
    "                        else:\n",
    "                            print(f\"Warning: {data_type} 데이터에서 파일을 찾을 수 없습니다. 경로: {audio_path}\")\n",
    "                \n",
    "                except KeyError as e:\n",
    "                    print(f\"Error: {data_type} 데이터의 JSON 파일 {json_path}에 필요한 키가 누락되었습니다: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {json_path}: {e}\")\n",
    "                    \n",
    "    # 오디오 파일 경로와 스크립트를 데이터프레임 형태로 변환\n",
    "    df = pd.DataFrame({\"path\": audio_paths, \"transcription\": transcriptions})\n",
    "\n",
    "    # 오디오 파일 경로와 스크립트가 저장된 데이터프레임을 Hugging Face의 Dataset 객체로 변환\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    print(f\"총 {len(dataset)}개의 {data_type}용 경제 도메인 데이터를 찾았습니다.\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_metrics(pred, processor, metric):\n",
    "    \"\"\"모델 성능(WER) 계산 함수\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"데이터 콜레이터: 배치 단위로 데이터 패딩 처리\"\"\"\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# prepare_dataset\n",
    "def prepare_dataset(batch, processor):\n",
    "    audio_file_path = batch[\"path\"]\n",
    "    \n",
    "    try:\n",
    "        # sf.read를 사용해 오디오 데이터를 직접 읽기\n",
    "        audio, sampling_rate = sf.read(audio_file_path)\n",
    "        \n",
    "        # Whisper 모델에 맞게 sampling_rate 재설정\n",
    "        if sampling_rate != 16000:\n",
    "            audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=16000)\n",
    "            sampling_rate = 16000\n",
    "            \n",
    "        # 특징 벡터 추출 및 토큰화\n",
    "        batch[\"input_features\"] = processor.feature_extractor(audio, sampling_rate=sampling_rate).input_features[0]\n",
    "        batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
    "        return batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오디오 파일 로딩 오류: {audio_file_path} - {e}\")\n",
    "        return {\"input_features\": None, \"labels\": None}\n",
    "\n",
    "def main():\n",
    "    \"\"\"전체 모델 훈련 및 평가 워크플로우를 실행하는 메인 함수\"\"\"\n",
    "    \n",
    "    # 데이터셋 최상위 경로 설정\n",
    "    base_data_path = \"C:/Users/Admin/Desktop/github/voice-recognition-using-whisper/data/news_scripts_and_speech_data/open_data\"\n",
    "\n",
    "    # 훈련 및 평가 여부를 설정하세요\n",
    "    run_training = False\n",
    "    run_evaluation = True\n",
    "\n",
    "    # 1. 데이터셋 로드 (조건부 로드)\n",
    "    train_dataset, eval_dataset = None, None\n",
    "\n",
    "    if run_training:\n",
    "        print(\"--- 훈련 데이터 로드 시작 ---\")\n",
    "        train_dataset = process_financial_news_data(base_data_path, \"Training\")\n",
    "        if train_dataset is None:\n",
    "            print(\"훈련 데이터 로드 실패. 프로그램을 종료합니다.\")\n",
    "            return\n",
    "\n",
    "    if run_evaluation:\n",
    "        print(\"\\n--- 검증 데이터 로드 시작 ---\")\n",
    "        eval_dataset = process_financial_news_data(base_data_path, \"Validation\")\n",
    "        if eval_dataset is None:\n",
    "            print(\"검증 데이터 로드 실패. 프로그램을 종료합니다.\")\n",
    "            return\n",
    "            \n",
    "    # 2. 모델, 프로세서 로드 (훈련 또는 평가가 필요한 경우에만)\n",
    "    if not (run_training or run_evaluation):\n",
    "        print(\"훈련 또는 평가 작업이 설정되지 않았습니다. 종료합니다.\")\n",
    "        return\n",
    "\n",
    "    model_name = \"openai/whisper-small\"\n",
    "    processor = WhisperProcessor.from_pretrained(model_name)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # 3. 데이터 전처리 (모델 로드 후)\n",
    "    if run_training:\n",
    "        print(\"\\n훈련 데이터 전처리 중입니다...\")\n",
    "        # map은 Hugging Face의 datasets 라이브러리에서 제공하는 매우 중요한 기능\n",
    "        # 데이터셋의 모든 개별 항목(행)에 대해 특정 함수를 효율적으로 적용하는 개념\n",
    "        # 코드에서 map의 역할\n",
    "        # train_dataset.map(...)은 train_dataset의 모든 행에 대해 prepare_dataset이라는 함수를 적용하라는 의미\n",
    "        train_dataset = train_dataset.map(\n",
    "            prepare_dataset, # 모든 데이터에 적용할 함수\n",
    "            remove_columns=train_dataset.column_names, # 변환 후 제거할 컬럼 설정\n",
    "            num_proc=os.cpu_count(), # 병렬 처리 설정 (시스템의 CPU 코어 수 사용)\n",
    "            fn_kwargs={\"processor\": processor} # prepare_dataset 함수에 전달할 추가 인자\n",
    "        ).filter(\n",
    "            lambda x: x[\"input_features\"] is not None\n",
    "        )\n",
    "        print(f\"필터링 후 훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "        print(\"훈련 데이터 전처리 완료.\")\n",
    "    \n",
    "    # main 함수 내에서 num_proc 변경\n",
    "    if run_evaluation:\n",
    "        print(\"\\n검증 데이터 전처리 중입니다...\")\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=eval_dataset.column_names,\n",
    "            num_proc=os.cpu_count(),\n",
    "            fn_kwargs={\"processor\": processor}\n",
    "        ).filter(\n",
    "            lambda x: x[\"input_features\"] is not None\n",
    "        )\n",
    "        print(f\"필터링 후 검증 데이터셋 크기: {len(eval_dataset)}\")\n",
    "        print(\"검증 데이터 전처리 완료.\")\n",
    "\n",
    "    # 4. 트레이너 설정 및 훈련/평가 실행\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./whisper-finetuned-finance\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=500,\n",
    "        max_steps=4000 if run_training else 0,\n",
    "        evaluation_strategy=\"steps\" if run_evaluation else \"no\",\n",
    "        eval_steps=500,\n",
    "        save_steps=1000,\n",
    "        fp16=True,\n",
    "        per_device_eval_batch_size=8,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        logging_steps=100,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "        do_train=run_training,\n",
    "        do_eval=run_evaluation,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda pred: compute_metrics(pred, processor, metric),\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    if run_training:\n",
    "        print(\"\\n학습을 시작합니다...\")\n",
    "        trainer.train()\n",
    "        print(\"학습 완료.\")\n",
    "\n",
    "    if run_evaluation:\n",
    "        print(\"\\n최종 모델을 평가합니다...\")\n",
    "        eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "        print(f\"최종 WER: {eval_results['eval_wer']:.2f}%\")\n",
    "        \n",
    "    if run_training:\n",
    "        final_model_path = \"./whisper-finetuned-finance/final_model\"\n",
    "        trainer.save_model(final_model_path)\n",
    "        processor.save_pretrained(final_model_path)\n",
    "        print(f\"최종 모델이 '{final_model_path}'에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
